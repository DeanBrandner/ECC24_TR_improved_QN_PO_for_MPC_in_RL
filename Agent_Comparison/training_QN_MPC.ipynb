{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'RL_Tools'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32mf:\\Arbeit\\Paper\\ECC24_TR_improved_QN_PO_for_MPC_in_RL\\Agent_Comparison\\training_QN_MPC.ipynb Cell 2\u001b[0m line \u001b[0;36m6\n\u001b[0;32m      <a href='vscode-notebook-cell:/f%3A/Arbeit/Paper/ECC24_TR_improved_QN_PO_for_MPC_in_RL/Agent_Comparison/training_QN_MPC.ipynb#W1sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mcasadi\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mcd\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/f%3A/Arbeit/Paper/ECC24_TR_improved_QN_PO_for_MPC_in_RL/Agent_Comparison/training_QN_MPC.ipynb#W1sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39menvironments\u001b[39;00m \u001b[39mimport\u001b[39;00m environment \u001b[39mas\u001b[39;00m Environment\n\u001b[1;32m----> <a href='vscode-notebook-cell:/f%3A/Arbeit/Paper/ECC24_TR_improved_QN_PO_for_MPC_in_RL/Agent_Comparison/training_QN_MPC.ipynb#W1sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mRL_Tools\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39magents\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mAC_MPC_agent_NN\u001b[39;00m \u001b[39mimport\u001b[39;00m AC_MPC_agent_NN_QuasiNewton \u001b[39mas\u001b[39;00m Agent\n\u001b[0;32m      <a href='vscode-notebook-cell:/f%3A/Arbeit/Paper/ECC24_TR_improved_QN_PO_for_MPC_in_RL/Agent_Comparison/training_QN_MPC.ipynb#W1sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mRL_Tools\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mtools\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mRL_AC_MPC\u001b[39;00m \u001b[39mimport\u001b[39;00m RL_AC_MPC \u001b[39mas\u001b[39;00m MPC\n\u001b[0;32m      <a href='vscode-notebook-cell:/f%3A/Arbeit/Paper/ECC24_TR_improved_QN_PO_for_MPC_in_RL/Agent_Comparison/training_QN_MPC.ipynb#W1sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mdo_mpc\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmodel\u001b[39;00m \u001b[39mimport\u001b[39;00m Model\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'RL_Tools'"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import os\n",
    "import casadi as cd\n",
    "\n",
    "from environments import environment as Environment\n",
    "from Reinforced_MPC.agents.AC_MPC_agent_NN import AC_MPC_agent_NN_QuasiNewton as Agent\n",
    "from Reinforced_MPC.tools.RL_AC_MPC import RL_AC_MPC as MPC\n",
    "\n",
    "from do_mpc.model import Model\n",
    "from matplotlib import pyplot as plt\n",
    "from sys import stdout\n",
    "\n",
    "from scipy.linalg import solve_discrete_are\n",
    "\n",
    "import tensorflow as tf\n",
    "tf.random.set_seed(0)\n",
    "\n",
    "from tensorflow import keras\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def progress_episodes(replay: int, max_replays: int, episode: int, episodes_per_replay: int, suffix = ''):\n",
    "    bar_len = 60\n",
    "    filled_len = int(round(bar_len * episode / int(episodes_per_replay)))\n",
    "\n",
    "    percents = round(100.0 * episode / int(episodes_per_replay), 1)\n",
    "    bar = '=' * filled_len + '-' * (bar_len - filled_len)\n",
    "\n",
    "    stdout.write(f'Replay {replay}/{max_replays},\\t[' + bar  + f'] {episode}/{episodes_per_replay} {percents}%\\r')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = f\"data\\\\agent\\\\QN_MPC\\\\\"\n",
    "fig_path = f\"figs\\\\agent\\\\QN_MPC\\\\\"\n",
    "if not os.path.exists(fig_path):\n",
    "    os.makedirs(fig_path)\n",
    "\n",
    "n_episodes = 2000\n",
    "steps_per_episode = 50\n",
    "n_batch = 50\n",
    "\n",
    "memory_size = 250\n",
    "shortterm_memory_size = n_batch\n",
    "n_samples = n_batch * steps_per_episode\n",
    "\n",
    "behave_rng_seed = 42\n",
    "exploration_rng_seed = 24\n",
    "tf_rng_seed = 99\n",
    "\n",
    "exploration_std = 1e-1\n",
    "epsilon_decay = (0.01/1)**(1/(n_episodes//memory_size))\n",
    "gamma = 1\n",
    "\n",
    "learning_rate = 1e-2\n",
    "normalize = False\n",
    "normalize_value = 1e0 * learning_rate\n",
    "\n",
    "training_hp = {\n",
    "    \"n_episodes\": n_episodes,\n",
    "    \"steps_per_episode\": steps_per_episode,\n",
    "    \"n_samples\": n_samples,\n",
    "    \"learning_rate\": learning_rate,\n",
    "    \"memory_size\": memory_size,\n",
    "    \"behave_rng_seed\": behave_rng_seed,\n",
    "    \"exploration_rng_seed\": exploration_rng_seed,\n",
    "    \"tf_rng_seed\": tf_rng_seed,\n",
    "    \"exploration_std\": exploration_std,\n",
    "    \"epsilon_decay\": epsilon_decay,\n",
    "    \"gamma\": gamma,\n",
    "    \"normalize\": normalize,\n",
    "    \"normalize_value\": normalize_value,\n",
    "}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build the model for the MPC_agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(\"discrete\")\n",
    "\n",
    "x = model.set_variable(var_type = \"_x\", var_name = \"x\", shape = (2,1))\n",
    "u = model.set_variable(var_type = \"_u\", var_name = \"u\", shape = (1,1))\n",
    "\n",
    "A11 = model.set_variable(var_type = \"_p\", var_name = \"A11\", shape = (1, 1))\n",
    "A12 = model.set_variable(var_type = \"_p\", var_name = \"A12\", shape = (1, 1))\n",
    "A22 = model.set_variable(var_type = \"_p\", var_name = \"A22\", shape = (1, 1))\n",
    "A = cd.vertcat(cd.horzcat(A11, A12), cd.horzcat(0, A22))\n",
    "B = model.set_variable(var_type = \"_p\", var_name = \"B\", shape = (2,1))\n",
    "bias = model.set_variable(var_type = \"_p\", var_name = \"bias\", shape = (2,1))\n",
    "lower_backoff = model.set_variable(var_type = \"_p\", var_name = \"lower_backoff\", shape = (1,1))\n",
    "\n",
    "\n",
    "x_next = A @ x + B @ u + bias\n",
    "model.set_rhs(\"x\", x_next)\n",
    "\n",
    "model.setup()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build the MPC based on the previous model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actor = MPC(model=model)\n",
    "\n",
    "Q_approx =  cd.diag(cd.DM([[1, 1]]))\n",
    "R_approx = cd.diag(cd.DM([[0.5]]))\n",
    "lterm = x.T @ Q_approx @ x + u.T @ R_approx @ u\n",
    "A_approx = cd.DM([[1,  0.25], [0, 1]])\n",
    "B_approx = cd.DM([[0.1], [0.3]])\n",
    "\n",
    "S = solve_discrete_are(A_approx, B_approx, Q_approx, R_approx)\n",
    "mterm = x.T @ S @ x\n",
    "actor.set_objective(lterm = lterm, mterm = mterm)\n",
    "\n",
    "actor.set_rterm(u = 0)\n",
    "\n",
    "actor.bounds[\"lower\", \"_u\", \"u\"] = -1\n",
    "actor.bounds[\"upper\", \"_u\", \"u\"] = +1\n",
    "\n",
    "x_lb = cd.DM([0, -1])\n",
    "x_ub = cd.DM([1, +1])\n",
    "x_lower_limit = x_lb + cd.vertcat(lower_backoff, cd.DM(0))\n",
    "x_upper_limit = x_ub\n",
    "actor.set_nl_cons(expr_name = \"x_lower_limit\", expr = +(x_lower_limit - x), ub = 0, soft_constraint = True, penalty_term_cons = 1e2) \n",
    "actor.set_nl_cons(expr_name = \"x_upper_limit\", expr = -(x_upper_limit - x), ub = 0, soft_constraint = True, penalty_term_cons = 1e2)\n",
    "\n",
    "actor.set_param(n_horizon = 10, t_step= 1, nlpsol_opts = {\"ipopt\": {\"print_level\": 0, \"tol\": 1e-8}, \"print_time\": False }, gamma = gamma)\n",
    "\n",
    "p_template = actor.get_p_template(1)\n",
    "p_template[\"_p\", 0, \"A11\"] = A_approx[0,0]\n",
    "p_template[\"_p\", 0, \"A12\"] = A_approx[0,1]\n",
    "p_template[\"_p\", 0, \"A22\"] = A_approx[1,1]\n",
    "p_template[\"_p\", 0, \"B\"] = B_approx\n",
    "actor.set_p_fun(lambda t_now: p_template)\n",
    "\n",
    "actor.setup()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build the Q-NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_layers  = 2\n",
    "n_units = 20\n",
    "Q_lr = 1e-3\n",
    "\n",
    "training_hp[\"n_layers\"] = n_layers\n",
    "training_hp[\"n_units\"] = n_units\n",
    "training_hp[\"Q_lr\"] = Q_lr\n",
    "\n",
    "critic_optimizer = keras.optimizers.AdamW(learning_rate = Q_lr)\n",
    "critic_loss = keras.losses.Huber()\n",
    "\n",
    "training_hp[\"critic_optimizer\"] = critic_optimizer.name\n",
    "training_hp[\"critic_loss\"] = critic_loss.name\n",
    "\n",
    "\n",
    "input_layer = keras.layers.Input(shape = x.shape[0] + u.shape[0] + u.shape[0])\n",
    "hidden = keras.layers.Dense(n_units, activation = \"tanh\")(input_layer)\n",
    "\n",
    "for idx in range(n_layers - 1):\n",
    "    hidden = keras.layers.Dense(n_units, activation = \"tanh\")(hidden)\n",
    "\n",
    "output_layer = keras.layers.Dense(1, activation = \"sigmoid\")(hidden)\n",
    "\n",
    "Q_NN = keras.Model(inputs = input_layer, outputs = output_layer, name = \"Q_NN\")\n",
    "\n",
    "Q_NN.summary()\n",
    "\n",
    "Q_NN.compile(\n",
    "    optimizer = critic_optimizer,\n",
    "    loss = critic_loss,\n",
    "    metrics = [\"mse\", \"mae\"]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "critic_training_kwargs = {\n",
    "    \"batch_size\": 256,\n",
    "    \"epochs\": 5000,\n",
    "    \"callbacks\": [keras.callbacks.EarlyStopping(monitor = \"loss\", patience = 50, restore_best_weights = True)],\n",
    "    \"verbose\": 0,\n",
    "    }"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build the AC_MPC_Agent based on the MPC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RL_settings = {\n",
    "    \"learning_rate\": learning_rate,\n",
    "    \"memory_size\": memory_size,\n",
    "    \"behave_rng_seed\": behave_rng_seed,\n",
    "    \"exploration_rng_seed\": exploration_rng_seed,\n",
    "    \"exploration_std\": exploration_std,\n",
    "    \"gamma\": gamma\n",
    "}\n",
    "n_critic_horizon = 10\n",
    "critic_settings = {\n",
    "    \"log_basis\": 10.,\n",
    "    \"log_offset\": 1e-4,\n",
    "}\n",
    "agent = Agent(\n",
    "    actor_mpc = actor,\n",
    "    critic = Q_NN,\n",
    "    n_critic_horizon = n_critic_horizon,\n",
    "    critic_settings = critic_settings,\n",
    "    **RL_settings)\n",
    "agent.save(save_path + \"untrained\\\\\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = Environment()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Untrained performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x0 = cd.DM([0.5, 0.5]).full()\n",
    "u_prev = cd.DM([0])\n",
    "\n",
    "env.set_initial_state(x0)\n",
    "\n",
    "stage_cost_list = []\n",
    "X_untrained = [x0]\n",
    "U_untrained = []\n",
    "for idx in range(steps_per_episode):\n",
    "    u = agent.act(x0)\n",
    "    x_next, stage_cost, done_flag = env.make_step(u)\n",
    "\n",
    "    x0 = x_next.copy()\n",
    "    u_prev = u.copy()\n",
    "\n",
    "    stage_cost_list.append(stage_cost)\n",
    "    X_untrained.append(x0)\n",
    "    U_untrained.append(u)\n",
    "\n",
    "stage_cost_list = cd.DM(stage_cost_list)\n",
    "X_untrained = cd.horzcat(*X_untrained).T\n",
    "U_untrained = cd.horzcat(*U_untrained).T\n",
    "\n",
    "agent.actor.reset_history()\n",
    "agent.actor_behavior.reset_history()\n",
    "agent.replay_mpc.reset_history()\n",
    "\n",
    "print(f\"Closed Loop cost: {stage_cost_list.full().sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(save_path + \"\\\\performance\"):\n",
    "    os.makedirs(save_path + \"\\\\performance\")\n",
    "\n",
    "with open(save_path + \"\\\\performance\\\\untrained_performance_X.pkl\", \"wb\") as f:\n",
    "    pickle.dump(X_untrained, f)\n",
    "    \n",
    "with open(save_path + \"\\\\performance\\\\untrained_performance_U.pkl\", \"wb\") as f:\n",
    "    pickle.dump(U_untrained, f)\n",
    "\n",
    "with open(save_path + \"\\\\performance\\\\untrained_performance_J.pkl\", \"wb\") as f:\n",
    "    pickle.dump(stage_cost_list, f)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot the untrained performance"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below you see the closed loop cost of the untrained MPC agent, given an initial condition of $x_0 = [0.5, 0.5]^\\top$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax_stage_cost = plt.subplots(nrows = 1, ncols = 1, constrained_layout = True)\n",
    "\n",
    "_ = ax_stage_cost.plot(stage_cost_list)\n",
    "\n",
    "_ = ax_stage_cost.set_xlabel(\"Time step\")\n",
    "_ = ax_stage_cost.set_ylabel(\"Stage cost\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below you see the trajectory of the untrained system, given an initial condition of $x_0 = [0.5, 0.5]^\\top$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax_trajectory_untrained = plt.subplots(nrows = 3, ncols = 1, constrained_layout = True)\n",
    "\n",
    "for axis in ax_trajectory_untrained:\n",
    "    axis.set_xlabel(\"Time step\")\n",
    "\n",
    "ax_trajectory_untrained[0].set_ylabel(r\"$x_1$\")\n",
    "ax_trajectory_untrained[1].set_ylabel(r\"$x_2$\")\n",
    "ax_trajectory_untrained[2].set_ylabel(r\"$u$\")\n",
    "\n",
    "ax_trajectory_untrained[0].plot(X_untrained[:,0])\n",
    "ax_trajectory_untrained[0].plot([0]*X_untrained.shape[0], \"--\", color = \"black\")\n",
    "\n",
    "ax_trajectory_untrained[1].plot(X_untrained[:,1])\n",
    "ax_trajectory_untrained[1].plot([0]*X_untrained.shape[0], \"--\", color = \"black\")\n",
    "\n",
    "ax_trajectory_untrained[2].step(x = range(U_untrained[:,0].shape[0]), y = U_untrained[:,0])\n",
    "ax_trajectory_untrained[2].plot([0]*U_untrained.shape[0], \"--\", color = \"black\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = [agent.actor.p_fun(0).master.T]\n",
    "cl_cost = []\n",
    "regularization_list = []\n",
    "\n",
    "u0_initial = agent.actor.u0.master.full().copy()\n",
    "\n",
    "env.reset()\n",
    "\n",
    "averaged_closed_loop_cost_list = []\n",
    "averaged_closed_loop_cost_loc_list = []\n",
    "best_closed_loop_cost = None\n",
    "counter = 0\n",
    "replay_count = 1\n",
    "\n",
    "\n",
    "for episode in range(n_episodes):\n",
    "    progress_episodes(replay_count, n_episodes//n_batch, counter + 1, n_batch)\n",
    "\n",
    "\n",
    "    if episode % n_batch == 0:\n",
    "        env.reset(random = False)\n",
    "    else:\n",
    "        env.reset(random = True)\n",
    "        \n",
    "    x0 = env.x_num.copy()\n",
    "    u_prev = u0_initial.copy()\n",
    "    agent.actor.u0.master = u_prev.copy()\n",
    "\n",
    "    agent.actor.reset_history()\n",
    "    agent.actor_behavior.reset_history()\n",
    "    agent.replay_mpc.reset_history()\n",
    "\n",
    "    closed_loop_cost = 0\n",
    "    \n",
    "    for step in range(steps_per_episode):\n",
    "        u0 = agent.behave(x0)\n",
    "        x_next, reward, done_flag = env.make_step(action = u0)\n",
    "        agent.remember(old_state = x0, old_action = u_prev, action = u0, reward = reward, next_state = x_next, done_flag = done_flag)\n",
    "        x0 = x_next.copy()\n",
    "        u_prev = u0.copy()\n",
    "\n",
    "        closed_loop_cost += reward\n",
    "\n",
    "    averaged_closed_loop_cost_loc_list.append(closed_loop_cost)\n",
    "    counter += 1\n",
    "    cl_cost.append(closed_loop_cost)\n",
    "    \n",
    "    agent.remember_episode()\n",
    "    \n",
    "    if counter < n_batch:\n",
    "        continue\n",
    "    \n",
    "    averaged_closed_loop_cost_loc = agent._get_closed_loop_cost()\n",
    "    if (best_closed_loop_cost is None) or (best_closed_loop_cost > averaged_closed_loop_cost_loc):\n",
    "        best_closed_loop_cost = averaged_closed_loop_cost_loc\n",
    "\n",
    "        agent.save(save_path)\n",
    "\n",
    "    averaged_closed_loop_cost_list.append(averaged_closed_loop_cost_loc)\n",
    "    averaged_closed_loop_cost_loc_list = []\n",
    "    counter = 0\n",
    "    \n",
    "    print()\n",
    "    \n",
    "    \n",
    "    print(f\"Averaged closed loop cost of this run: {averaged_closed_loop_cost_list[-1]:.3f} \\t Best average closed loop cost so far: {best_closed_loop_cost:.3f}\")\n",
    "    \n",
    "    \n",
    "    continue_bool = agent.replay(critic_training_kwargs = critic_training_kwargs)\n",
    "    if not continue_bool:\n",
    "        break\n",
    "\n",
    "    replay_count += 1\n",
    "    \n",
    "    agent.decay_epsilon()\n",
    "    parameters.append(agent.actor.p_fun(0).master.T)\n",
    "\n",
    "    print(\"Agent parameters: {}\".format(agent.actor.p_fun(0)[\"_p\", 0]))\n",
    "    print()\n",
    "parameters = cd.vertcat(*parameters)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = Agent.load(save_path)\n",
    "untrained_agent = Agent.load(save_path + \"untrained\\\\\")\n",
    "print(\"Agent parameters: {}\".format(agent.actor.p_fun(0)[\"_p\", 0]))\n",
    "print(\"Agent parameters (untrained): {}\".format(untrained_agent.actor.p_fun(0)[\"_p\", 0]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save the model parameters and the MPC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.save(save_path  + \"agent\")\n",
    "\n",
    "with open(save_path + \"training_hp.pkl\", \"wb\") as f:\n",
    "    pickle.dump(training_hp, f)\n",
    "\n",
    "with open(save_path + \"parameters.pkl\", \"wb\") as f:\n",
    "    pickle.dump(parameters, f)\n",
    "\n",
    "with open(save_path + \"cl_cost.pkl\", \"wb\") as f:\n",
    "    pickle.dump(cl_cost, f)\n",
    "\n",
    "with open(save_path + \"regularization_list.pkl\", \"wb\") as f:\n",
    "    pickle.dump(regularization_list, f)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build the MPC benchmark\n",
    "The benchmark is an MPC with the exact model and full prediction horizon.\n",
    "\n",
    "First the exact model is setup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exact_model = Model(\"discrete\")\n",
    "\n",
    "x_exact = exact_model.set_variable(var_type = \"_x\", var_name = \"x\", shape = (2,1))\n",
    "u_exact = exact_model.set_variable(var_type = \"_u\", var_name = \"u\", shape = (1,1))\n",
    "\n",
    "A_exact = cd.DM([[0.9,  0.35], [0, 1.1]])\n",
    "B_exact = cd.DM([[0.0813], [0.2]])\n",
    "\n",
    "x_next_exact = A_exact @ x_exact + B_exact @ u_exact\n",
    "exact_model.set_rhs(\"x\", x_next_exact)\n",
    "\n",
    "exact_model.setup()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the benchmark MPC is set up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "benchmark_MPC = MPC(model = exact_model)\n",
    "\n",
    "q_exact = cd.DM([1, 1])\n",
    "lterm = x_exact.T @ cd.diag(q_exact) @ x_exact + 0.5 * u_exact.T @ u_exact\n",
    "\n",
    "S_exact = solve_discrete_are(A_exact, B_exact, cd.diag(q_exact), cd.DM(0.5))\n",
    "mterm = x_exact.T @ S_exact @ x_exact\n",
    "benchmark_MPC.set_objective(lterm = lterm, mterm = mterm)\n",
    "\n",
    "benchmark_MPC.set_rterm(u = 0)\n",
    "\n",
    "benchmark_MPC.bounds[\"lower\", \"_u\", \"u\"] = -1\n",
    "benchmark_MPC.bounds[\"upper\", \"_u\", \"u\"] = +1\n",
    "\n",
    "x_lb_exact = cd.DM([0, -1])\n",
    "x_ub_exact = cd.DM([1, +1])\n",
    "\n",
    "benchmark_MPC.set_nl_cons(expr_name = \"x_lower_limit\", expr = x_lb_exact +(- x_exact), ub = 0, soft_constraint = True, penalty_term_cons = 1e2)\n",
    "benchmark_MPC.set_nl_cons(expr_name = \"x_upper_limit\", expr = x_exact - x_ub_exact, ub = 0, soft_constraint = True, penalty_term_cons = 1e2)\n",
    "\n",
    "benchmark_MPC.set_param(n_horizon = 50, t_step= 1, nlpsol_opts = {\"ipopt\": {\"print_level\": 0, \"tol\": 1e-6}, \"print_time\": False }, gamma = gamma)\n",
    "\n",
    "\n",
    "benchmark_MPC.setup()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the benchmark MPC on the actual system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.reset()\n",
    "\n",
    "x0 = cd.DM([0.5, 0.5]).full()\n",
    "u_prev = cd.DM([0])\n",
    "\n",
    "env.set_initial_state(x0)\n",
    "\n",
    "stage_cost_benchmark_list = []\n",
    "X_benchmark = [x0]\n",
    "U_benchmark = []\n",
    "for idx in range(steps_per_episode):\n",
    "    u = benchmark_MPC.make_step(x0)\n",
    "    x_next, stage_cost, done_flag = env.make_step(u)\n",
    "\n",
    "    x0 = x_next.copy()\n",
    "    u_prev = u.copy()\n",
    "\n",
    "    stage_cost_benchmark_list.append(stage_cost)\n",
    "    X_benchmark.append(x0)\n",
    "    U_benchmark.append(u)\n",
    "\n",
    "stage_cost_benchmark_list = cd.DM(stage_cost_benchmark_list)\n",
    "X_benchmark = cd.horzcat(*X_benchmark).T\n",
    "U_benchmark = cd.horzcat(*U_benchmark).T\n",
    "\n",
    "print(f\"Closed Loop cost: {stage_cost_benchmark_list.full().sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_env = Environment(seed = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "closed_loop_cost_benchmark_list = []\n",
    "for episode in range(n_batch):\n",
    "    if episode == 0:\n",
    "        env.reset(random = False)\n",
    "    else:\n",
    "        env.reset(random = True)\n",
    "\n",
    "    x0 = env.x_num.copy()\n",
    "    u_prev = cd.DM([0])\n",
    "\n",
    "    closed_loop_cost_benchmark = 0\n",
    "    for idx in range(steps_per_episode):\n",
    "        u = benchmark_MPC.make_step(x0)\n",
    "        x_next, stage_cost, done_flag = env.make_step(u)\n",
    "\n",
    "        x0 = x_next.copy()\n",
    "        u_prev = u.copy()\n",
    "\n",
    "        closed_loop_cost_benchmark += stage_cost\n",
    "\n",
    "    closed_loop_cost_benchmark_list.append(closed_loop_cost_benchmark)\n",
    "\n",
    "average_closed_loop_cost_benchmark = sum(closed_loop_cost_benchmark_list)/len(closed_loop_cost_benchmark_list)\n",
    "print(f\"Averaged closed Loop cost {n_batch} runs: {average_closed_loop_cost_benchmark:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(save_path + \"\\\\performance\"):\n",
    "    os.makedirs(save_path + \"\\\\performance\")\n",
    "\n",
    "with open(save_path + \"\\\\performance\\\\benchmark_performance_X.pkl\", \"wb\") as f:\n",
    "    pickle.dump(X_benchmark, f)\n",
    "    \n",
    "with open(save_path + \"\\\\performance\\\\benchmark_performance_U.pkl\", \"wb\") as f:\n",
    "    pickle.dump(U_benchmark, f)\n",
    "\n",
    "with open(save_path + \"\\\\performance\\\\benchmark_performance_J.pkl\", \"wb\") as f:\n",
    "    pickle.dump(stage_cost_benchmark_list, f)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trained performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "average_closed_loop_cost_trained = 0\n",
    "for episode in range(n_batch):\n",
    "    if episode == 0:\n",
    "        env.reset(random = False)\n",
    "    else:\n",
    "        env.reset(random = True)\n",
    "\n",
    "    x0 = env.x_num.copy()\n",
    "    u_prev = cd.DM([0])\n",
    "\n",
    "    stage_cost_trained_list = []\n",
    "    X_trained = [x0]\n",
    "    U_trained = []\n",
    "    for idx in range(steps_per_episode):\n",
    "        u = agent.act(x0)\n",
    "        x_next, stage_cost, done_flag = env.make_step(u)\n",
    "\n",
    "        x0 = x_next.copy()\n",
    "        u_prev = u.copy()\n",
    "\n",
    "        stage_cost_trained_list.append(stage_cost)\n",
    "        X_trained.append(x0)\n",
    "        U_trained.append(u)\n",
    "\n",
    "    stage_cost_trained_list = cd.DM(stage_cost_trained_list)\n",
    "    X_trained = cd.horzcat(*X_trained).T\n",
    "    U_trained = cd.horzcat(*U_trained).T\n",
    "\n",
    "    average_closed_loop_cost_trained = episode/(episode+1) * average_closed_loop_cost_trained + 1/(episode+1) * stage_cost_trained_list.full().sum()\n",
    "\n",
    "print(f\"Averaged closed Loop cost {n_batch} runs: {average_closed_loop_cost_trained:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.reset()\n",
    "\n",
    "agent.actor.reset_history()\n",
    "agent.actor_behavior.reset_history()\n",
    "agent.replay_mpc.reset_history()\n",
    "\n",
    "x0 = cd.DM([0.5, 0.5]).full()\n",
    "u_prev = cd.DM([0])\n",
    "\n",
    "env.set_initial_state(x0)\n",
    "\n",
    "stage_cost_trained_list = []\n",
    "X_trained = [x0]\n",
    "U_trained = []\n",
    "for idx in range(steps_per_episode):\n",
    "    u = agent.act(x0)\n",
    "    x_next, stage_cost, done_flag = env.make_step(u)\n",
    "\n",
    "    x0 = x_next.copy()\n",
    "    u_prev = u.copy()\n",
    "\n",
    "    stage_cost_trained_list.append(stage_cost)\n",
    "    X_trained.append(x0)\n",
    "    U_trained.append(u)\n",
    "\n",
    "stage_cost_trained_list = cd.DM(stage_cost_trained_list)\n",
    "X_trained = cd.horzcat(*X_trained).T\n",
    "U_trained = cd.horzcat(*U_trained).T\n",
    "\n",
    "agent.actor.reset_history()\n",
    "agent.actor_behavior.reset_history()\n",
    "agent.replay_mpc.reset_history()\n",
    "\n",
    "print(f\"Closed Loop cost: {stage_cost_trained_list.full().sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(save_path + \"\\\\performance\"):\n",
    "    os.makedirs(save_path + \"\\\\performance\")\n",
    "\n",
    "with open(save_path + \"\\\\performance\\\\trained_performance_X.pkl\", \"wb\") as f:\n",
    "    pickle.dump(X_trained, f)\n",
    "    \n",
    "with open(save_path + \"\\\\performance\\\\trained_performance_U.pkl\", \"wb\") as f:\n",
    "    pickle.dump(U_trained, f)\n",
    "\n",
    "with open(save_path + \"\\\\performance\\\\trained_performance_J.pkl\", \"wb\") as f:\n",
    "    pickle.dump(stage_cost_trained_list, f)\n",
    "\n",
    "with open(save_path + \"\\\\performance\\\\averaged_cl_cost_over_episodes.pkl\", \"wb\") as f:\n",
    "    pickle.dump(averaged_closed_loop_cost_list, f)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot the trained performance"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below you see the closed loop cost of the trained MPC agent, given an initial condition of $x_0 = [0.5, 0.5]^\\top$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax_stage_cost_trained = plt.subplots(nrows = 1, ncols = 1, constrained_layout = True)\n",
    "\n",
    "_ = ax_stage_cost_trained.plot(stage_cost_benchmark_list, label = \"benchmark\")\n",
    "_ = ax_stage_cost_trained.plot(stage_cost_trained_list, label = \"AC_MPC\")\n",
    "\n",
    "_ = ax_stage_cost_trained.set_xlabel(\"Time step\")\n",
    "_ = ax_stage_cost_trained.set_ylabel(\"Stage cost\")\n",
    "\n",
    "_ = ax_stage_cost_trained.legend(loc = \"upper right\")\n",
    "\n",
    "plt.savefig(fig_path + \"stage_cost_comparison.png\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below you see the trajectory of the trained system, given an initial condition of $x_0 = [0.5, 0.5]^\\top$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax_trajectory_trained = plt.subplots(nrows = 3, ncols = 1, constrained_layout = True)\n",
    "\n",
    "for axis in ax_trajectory_trained:\n",
    "    axis.set_xlabel(\"Time step\")\n",
    "\n",
    "_ = ax_trajectory_trained[0].set_ylabel(r\"$x_1$\")\n",
    "_ = ax_trajectory_trained[1].set_ylabel(r\"$x_2$\")\n",
    "_ = ax_trajectory_trained[2].set_ylabel(r\"$u$\")\n",
    "\n",
    "_ = ax_trajectory_trained[0].plot(X_benchmark[:,0], label = \"benchmark\")\n",
    "_ = ax_trajectory_trained[0].plot(X_untrained[:,0], label = \"untrained\")\n",
    "_ = ax_trajectory_trained[0].plot(X_trained[:,0], label = \"trained\")\n",
    "_ = ax_trajectory_trained[0].plot([0]*X_trained.shape[0], \"--\", color = \"black\")\n",
    "_ = ax_trajectory_trained[0].legend(loc = \"upper right\")\n",
    "\n",
    "_ = ax_trajectory_trained[1].plot(X_benchmark[:,1], label = \"benchmark\")\n",
    "_ = ax_trajectory_trained[1].plot(X_untrained[:,1], label = \"untrained\")\n",
    "_ = ax_trajectory_trained[1].plot(X_trained[:,1], label = \"trained\")\n",
    "_ = ax_trajectory_trained[1].plot([0]*X_trained.shape[0], \"--\", color = \"black\")\n",
    "\n",
    "_ = ax_trajectory_trained[2].step(x = range(U_benchmark[:,0].shape[0]), y = U_benchmark[:,0], label = \"benchmark\", where = \"post\")\n",
    "_ = ax_trajectory_trained[2].step(x = range(U_untrained[:,0].shape[0]), y = U_untrained[:,0], label = \"untrained\",  where = \"post\")\n",
    "_ = ax_trajectory_trained[2].step(x = range(U_trained[:,0].shape[0]), y = U_trained[:,0], label = \"trained\",  where = \"post\")\n",
    "_ = ax_trajectory_trained[2].plot([0]*U_trained.shape[0], \"--\", color = \"black\")\n",
    "\n",
    "plt.savefig(fig_path + \"benchmark_vs_trained.png\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below you see the training trajectory of the parameters of the trained system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nrows = 2\n",
    "ncols = 4\n",
    "\n",
    "k = 0.75\n",
    "\n",
    "fig, ax_parameters = plt.subplots(nrows = nrows, ncols = ncols, figsize = (k * 5 * ncols, k * 4 * nrows), constrained_layout = True)\n",
    "\n",
    "for axis in ax_parameters.flatten():\n",
    "    axis.set_xlabel(\"Update step\")\n",
    "\n",
    "_ = ax_parameters[0, 0].set_ylabel(r\"$A_{11}$\")\n",
    "_ = ax_parameters[0, 1].set_ylabel(r\"$A_{12}$\")\n",
    "_ = ax_parameters[1, 1].set_ylabel(r\"$A_{22}$\")\n",
    "_ = ax_parameters[0, 2].set_ylabel(r\"$B_{11}$\")\n",
    "_ = ax_parameters[1, 2].set_ylabel(r\"$B_{12}$\")\n",
    "_ = ax_parameters[0, 3].set_ylabel(r\"$b$\")\n",
    "_ = ax_parameters[1, 3].set_ylabel(r\"$\\Delta x$\")\n",
    "\n",
    "_ = ax_parameters[0, 0].plot(parameters[:, 0])\n",
    "_ = ax_parameters[0, 1].plot(parameters[:, 1])\n",
    "# _ = ax_parameters[1, 0].plot(parameters[:, 1 - 1])\n",
    "_ = ax_parameters[1, 1].plot(parameters[:, 2])\n",
    "_ = ax_parameters[0, 2].plot(parameters[:, 4 - 1])\n",
    "_ = ax_parameters[1, 2].plot(parameters[:, 5 - 1])\n",
    "_ = ax_parameters[0, 3].plot(parameters[:, 6 - 1])\n",
    "_ = ax_parameters[0, 3].plot(parameters[:, 7 - 1])\n",
    "_ = ax_parameters[1, 3].plot(parameters[:, 8 - 1])\n",
    "\n",
    "plt.savefig(fig_path + \"parameters.png\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Closed loop cost over the episodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax_averaged_cl_cost_over_episodes = plt.subplots(nrows = 1, ncols = 1, constrained_layout = True)\n",
    "\n",
    "_ = ax_averaged_cl_cost_over_episodes.plot([average_closed_loop_cost_benchmark]*len(averaged_closed_loop_cost_list), label = \"benchmark\")\n",
    "_ = ax_averaged_cl_cost_over_episodes.plot(averaged_closed_loop_cost_list, label = \"AC_MPC_QN\")\n",
    "\n",
    "_ = ax_averaged_cl_cost_over_episodes.set_xlabel(\"Episode\")\n",
    "_ = ax_averaged_cl_cost_over_episodes.set_ylabel(\"Averaged closed loop cost\")\n",
    "\n",
    "_ = ax_averaged_cl_cost_over_episodes.legend(loc = \"upper right\")\n",
    "\n",
    "_ = ax_averaged_cl_cost_over_episodes.set_ylim([10, 30])\n",
    "\n",
    "\n",
    "plt.savefig(fig_path + \"averaged_cl_cost_over_episodes.png\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RL_Test_Environments",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
